{
    "code": "import pandas as pd\nimport os\nimport io\nimport s3fs\nimport pyarrow\n\n# Define the output directory\noutput_dir = \"uploads/75c643ba-923f-473b-b566-210d7f249e4d\"\nos.makedirs(output_dir, exist_ok=True)\n\n# S3 bucket path pattern as given in the DuckDB query\ns3_path_pattern = \"s3://indian-high-court-judgments/metadata/parquet/year=*/court=*/bench=*/metadata.parquet\"\n\n# Read data from S3 using pandas.read_parquet\n# pandas internally uses fsspec to handle S3 paths and glob patterns\n# The storage_options dictionary is passed directly to fsspec for S3 configuration\ntry:\n    df = pd.read_parquet(\n        s3_path_pattern,\n        storage_options={\n            'client_kwargs': {'region_name': 'ap-south-1'}\n        }\n    )\nexcept Exception as e:\n    # In case of an error (e.g., S3 access denied, no files found, network issue),\n    # create an empty DataFrame to allow metadata collection to proceed without crashing.\n    print(f\"Error reading Parquet from S3: {e}\")\n    # Define columns based on the provided schema to ensure consistency for metadata.txt\n    columns = [\n        'court_code', 'title', 'description', 'judge', 'pdf_link', 'cnr',\n        'date_of_registration', 'decision_date', 'disposal_nature', 'court',\n        'raw_html', 'bench', 'year'\n    ]\n    df = pd.DataFrame(columns=columns)\n\n\n# Save the DataFrame to CSV\ndata_csv_path = os.path.join(output_dir, \"data.csv\")\ndf.to_csv(data_csv_path, index=False)\n\n# Collect metadata\nmetadata_txt_path = os.path.join(output_dir, \"metadata.txt\")\nwith open(metadata_txt_path, \"w\") as f:\n    f.write(\"---\u00a0DataFrame Info ---\\n\")\n    # Redirect df.info() output to a string buffer and write to file\n    buf = io.StringIO()\n    df.info(buf=buf)\n    f.write(buf.getvalue())\n    f.write(\"\\n\\n---\u00a0DataFrame Columns ---\\n\")\n    f.write(str(df.columns.tolist()))\n    f.write(\"\\n\\n---\u00a0DataFrame Head ---\\n\")\n    f.write(df.head().to_string())\n    f.write(\"\\n\\n---\u00a0Data CSV Path ---\\n\")\n    f.write(f\"Path: {data_csv_path}\\n\")\n    f.write(\"Description: Scraped judicial judgment metadata from S3 bucket.\\n\")\n    f.write(\"\\n\\n---\u00a0Original Questions JSON Format (for reference) ---\\n\")\n    f.write(\"\"\"{\n  \"Which high court disposed the most cases from 2019 - 2022?\": \"...\",\n  \"What's the regression slope of the date_of_registration - decision_date by year in the court=33_10?\": \"...\",\n  \"Plot the year and # of days of delay from the above question as a scatterplot with a regression line. Encode as a base64 data URI under 100,000 characters\": \"data:image/webp:base64,...\"\n}\"\"\")\n",
    "libraries": [
        "pandas",
        "s3fs",
        "pyarrow"
    ],
    "questions": [
        "Which high court disposed the most cases from 2019 - 2022?",
        "What's the regression slope of the date_of_registration - decision_date by year in the court=33_10?",
        "Plot the year and # of days of delay from the above question as a scatterplot with a regression line. Encode as a base64 data URI under 100,000 characters"
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}